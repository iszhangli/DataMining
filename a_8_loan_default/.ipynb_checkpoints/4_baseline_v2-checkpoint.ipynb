{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rocky-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data_dir = 'C:/ZhangLI/Codes/DataSet/个人违贷/official_data/'\n",
    "train_pub = pd.read_csv(data_dir+'train_public.csv')\n",
    "train_net = pd.read_csv(data_dir+'train_internet.csv')\n",
    "test_pub = pd.read_csv(data_dir+'test_public.csv')\n",
    "\n",
    "\n",
    "drop_cols = ['earlies_credit_mon', 'policy_code', 'user_id', 'loan_id', 'scoring_high', 'scoring_low']\n",
    "train_net['isDefault'] = train_net['is_default']\n",
    "common_feature = list(set(train_pub.columns).intersection(set(train_net.columns)))\n",
    "common_feature = [col for col in common_feature if col not in drop_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acute-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(dataframe):\n",
    "    dataframe['issue_date'] = pd.to_datetime(dataframe['issue_date'])\n",
    "    dataframe['issue_date_y'] = dataframe['issue_date'].dt.year\n",
    "    dataframe['issue_date_m'] = dataframe['issue_date'].dt.month\n",
    "\n",
    "    # set origin date\n",
    "    # get the diff bewteen now and origin-date\n",
    "    origin_date = datetime.datetime.strptime('2007-06-01', '%Y-%m-%d')\n",
    "    dataframe['issue_date_diff'] = dataframe['issue_date'].apply(lambda x: x-origin_date).dt.days\n",
    "    dataframe.drop('issue_date', axis = 1, inplace = True)\n",
    "    \n",
    "    # 就业类型\n",
    "    employer_type = dataframe['employer_type'].value_counts().index\n",
    "    industry = dataframe['industry'].value_counts().index\n",
    "    emp_type_dict = dict(zip(employer_type, [0,1,2,3,4,5]))\n",
    "    industry_dict = dict(zip(industry, [i for i in range(15)]))\n",
    "    # \n",
    "    dataframe['work_year'].fillna('10+ years', inplace=True)\n",
    "\n",
    "    work_year_map = {'10+ years': 10, '2 years': 2, '< 1 year': 0, '3 years': 3, '1 year': 1,\n",
    "         '5 years': 5, '4 years': 4, '6 years': 6, '8 years': 8, '7 years': 7, '9 years': 9}\n",
    "    dataframe['work_year']  = dataframe['work_year'].map(work_year_map)\n",
    "\n",
    "    dataframe['class'] = dataframe['class'].map({'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6})\n",
    "\n",
    "    emp_type_dict = {'政府机构': 1, '幼教与中小学校': 2, '高等教育机构': 3, '世界五百强': 4, '上市企业': 5, '普通企业': 6}\n",
    "    dataframe['employer_type'] = dataframe['employer_type'].map(emp_type_dict)\n",
    "\n",
    "    dataframe['industry'] = dataframe['industry'].map(industry_dict)\n",
    "    return dataframe\n",
    "\n",
    "def find_outliers_by_3segama(dataframe=None, features=None, label=None, verbose=False, is_drop=False):\n",
    "    # features are numerical type.\n",
    "    numerical_col = features\n",
    "    for col in numerical_col:\n",
    "        col_std = np.std(dataframe[col])\n",
    "        col_mean = np.mean(dataframe[col])\n",
    "        outliers_cut_off = col_std * 3\n",
    "        lower_rule = col_mean - outliers_cut_off\n",
    "        upper_rule = col_mean + outliers_cut_off\n",
    "        dataframe[col + '_outliers'] = dataframe[col].apply(lambda x:str('异常值') if x > upper_rule or x < lower_rule else '正常值')\n",
    "        if verbose:\n",
    "            print(dataframe[col + '_outliers'].value_counts())\n",
    "            print('-'*35)\n",
    "            print(dataframe.groupby(col + '_outliers')['isDefault'].sum())\n",
    "            print('='*50)\n",
    "    if is_drop:\n",
    "        for col in numerical_col:\n",
    "            dataframe = dataframe[dataframe[col + '_outliers']=='正常值']\n",
    "            dataframe = dataframe.reset_index(drop=True)\n",
    "            dataframe = dataframe.drop(col+'_outliers', axis=1)\n",
    "    return dataframe\n",
    "\n",
    "def get_mean_of_COL(train_df=None, test_df=None, cols=[], label='label', verbose=False):\n",
    "    # get mean of col about the label\n",
    "    for col in cols:\n",
    "        if verbose:\n",
    "            print(f'Get mean of {col} about the label.')\n",
    "        df_dict = train_df.groupby([col])[label].agg(['mean']).reset_index()\n",
    "        df_dict.index = df_dict[col].values\n",
    "        dict_col = df_dict['mean'].to_dict()\n",
    "        train_df[col+'_mean'] = train_df[col].map(dict_col)\n",
    "        test_df[col+'_mean'] = test_df[col].map(dict_col)\n",
    "    return train_df, test_df\n",
    "\n",
    "def create_feature(dataframe):\n",
    "    dataframe['is_early_return'] = (dataframe['early_return'] > 0).astype('int')\n",
    "    # 贷款时长\n",
    "    dataframe['return_duration'] = dataframe['total_loan'] / dataframe['monthly_payment']\n",
    "    # 近3个月是否提前还款\n",
    "    dataframe['is_early_return_amount_3mon'] = (dataframe['early_return_amount_3mon'] > 0).astype('int')\n",
    "    # 总还款金额 占 贷款多少\n",
    "    dataframe['early_return_ration'] = dataframe['early_return_amount'] / dataframe['total_loan']\n",
    "    # 总还款金额占每个月还款金额的多少\n",
    "    dataframe['early_return_monthly_ration'] = dataframe['early_return_amount'] / dataframe['monthly_payment']\n",
    "    return dataframe\n",
    "\n",
    "def get_mean_std_of_CAC(dataframe=None, cols1=[], cols2=[], slience=False):\n",
    "    # get mean/std of feature about another feature\n",
    "    for col1 in cols1:\n",
    "        for col2 in cols2:\n",
    "            if slience:\n",
    "                print(f'Get the mean/std. Ex: groupby(\\'{col1}\\')[\\'{col2}\\'].transform(\\' \\')')\n",
    "            dataframe[col1+'_'+ col2+'_mean'] = dataframe.groupby([col1])[col2].transform('mean')\n",
    "            dataframe[col1+'_'+ col2+'_std'] = dataframe.groupby([col1])[col2].transform('std')\n",
    "            dataframe[col1+'_'+ col2+'_mean_c'] = dataframe[col2] - dataframe[col1+'_'+ col2+'_mean']\n",
    "    return dataframe    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "positive-detector",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'work_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f27abd17dc93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mclass_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'use'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'work_type'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'work_year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mmean_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'total_loan'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'monthly_payment'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'interest'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mtrain_df_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_mean_std_of_CAC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;31m# ======================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-05d937af47a4>\u001b[0m in \u001b[0;36mget_mean_std_of_CAC\u001b[1;34m(dataframe, cols1, cols2, slience)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mslience\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Get the mean/std. Ex: groupby(\\'{col1}\\')[\\'{col2}\\'].transform(\\' \\')'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mcol2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_mean'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_pub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m             \u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mcol2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_std'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_pub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'std'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mcol2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_mean_c'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mcol2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\zhangli\\software\\installer\\python38\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   6715\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6717\u001b[1;33m         return DataFrameGroupBy(\n\u001b[0m\u001b[0;32m   6718\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6719\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\zhangli\\software\\installer\\python38\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    558\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_grouper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[0;32m    561\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\zhangli\\software\\installer\\python38\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    809\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m             \u001b[1;31m# Add key to exclusions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'work_type'"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "train_pub_new = copy.deepcopy(train_pub)\n",
    "train_net_new = copy.deepcopy(train_net)\n",
    "train_pub_new = train_pub_new[common_feature]\n",
    "train_net_new = train_net_new[common_feature]\n",
    "# 特征处理\n",
    "train_df = pd.concat([train_pub_new, train_net_new])\n",
    "# 学习特征处理\n",
    "numerical_col = list(train_df.select_dtypes(exclude=['object']).columns)\n",
    "# category_col = list(filter(lambda x: x not in numerical_col,list(train_df.columns)))\n",
    "# train_df[numerical_col] = train_df[numerical_col].fillna(train_df[numerical_col].median())\n",
    "# train_df[category_col].fillna(train_df[category_col].mode())\n",
    "\n",
    "train_df = find_outliers_by_3segama(dataframe=train_df, features=numerical_col, label='isDefault', verbose=False, is_drop=True)\n",
    "# -------------------------\n",
    "train_df_new = preprocessor(train_df)\n",
    "train_df_new = create_feature(train_df_new)\n",
    "class_cols = ['class', 'use', 'industry', 'work_year']\n",
    "mean_cols = ['total_loan', 'monthly_payment', 'interest']\n",
    "train_df_new = get_mean_std_of_CAC(train_df_new, class_cols, mean_cols)\n",
    "# ======================================\n",
    "\n",
    "X_train = train_df_new.drop('isDefault', axis=1)\n",
    "y_train = train_df_new['isDefault']\n",
    "train_x, val_x, train_y, val_y = train_test_split(X_train, y_train, test_size=0.2)\n",
    "train_matrix = lgb.Dataset(train_x, label=train_y)\n",
    "valid_matrix = lgb.Dataset(val_x, label=val_y)\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'class_weight':'balanced',\n",
    "            'objective': 'binary',\n",
    "            'learning_rate': 0.01,\n",
    "            'metric': 'auc',\n",
    "            'min_child_weight': 1e-3,\n",
    "            'num_leaves': 15,\n",
    "            'max_depth': 12,\n",
    "            'reg_lambda': 0.5,\n",
    "            'reg_alpha': 0.5,\n",
    "            'feature_fraction': 1,\n",
    "            'bagging_fraction': 1,\n",
    "            'bagging_freq': 0,\n",
    "            'seed': 2020,\n",
    "            'nthread': 8,\n",
    "            'silent': True,\n",
    "            'verbose': -1,\n",
    "            'subsample': 0.5\n",
    "}\n",
    "\n",
    "model = lgb.train(params, train_set=train_matrix, valid_sets=valid_matrix, num_boost_round=400, verbose_eval=100, early_stopping_rounds=100)\n",
    "\n",
    "\n",
    "val_pred_lgb = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "fpr, tpr, threshold = metrics.roc_curve(val_y, val_pred_lgb)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, 'b', label = 'Val AUC = %0.4f' % roc_auc)\n",
    "plt.plot([0, 1], [0,1], 'r--')\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------\n",
    "common_feature.remove('isDefault')\n",
    "test_pub = test_pub[common_feature]\n",
    "test_pub_new = copy.deepcopy(test_pub)\n",
    "\n",
    "# test_df_new[numerical_col] = test_df_new[numerical_col].fillna(train_df[numerical_col].median())\n",
    "# test_df_new[category_col].fillna(test_df_new[category_col].mode())\n",
    "test_df_new = preprocessor(test_df_new)\n",
    "test_df_new = create_feature(test_df_new)\n",
    "\n",
    "test_df_new = get_mean_std_of_CAC(test_df_new, class_cols, mean_cols)\n",
    "# # 模型预测\n",
    "predict = model.predict(test_df_new, num_iteration=model.best_iteration)\n",
    "# # 生成结果\n",
    "submission = pd.DataFrame({'id':test_pub['loan_id'], 'isDefault':predict})\n",
    "# submission.to_csv('submission.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score, confusion_matrix, average_precision_score\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "def validation_prediction_lgb(X, y, feature_names, ratio=1, X_test=None, y_test=None, istest=False):\n",
    "    n_fold = 5\n",
    "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': -1,\n",
    "        'metric': 'auc',\n",
    "        'is_unbalance': False,\n",
    "        'boost_from_average': False,\n",
    "    }\n",
    "\n",
    "    importances = pd.DataFrame()\n",
    "    if istest:\n",
    "        prediction = np.zeros(len(X_test))\n",
    "    models = []\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_train, X_valid = X[train_index], X[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        weights = [ratio if val == 1 else 1 for val in y_train]\n",
    "        train_data = lgb.Dataset(X_train, label=y_train, weight=weights)  # free_raw_data=True\n",
    "        valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "        model = lgb.train(params, train_data, num_boost_round=1000,\n",
    "                          valid_sets=[train_data, valid_data], verbose_eval=250, early_stopping_rounds=100)\n",
    "        \n",
    "        imp_df = pd.DataFrame()\n",
    "        imp_df['feature'] = feature_names\n",
    "        imp_df['split'] = model.feature_importance()\n",
    "        imp_df['gain'] = model.feature_importance(importance_type='gain')\n",
    "        imp_df['fold'] = fold_n + 1\n",
    "        importances = pd.concat([importances, imp_df], axis=0)\n",
    "        models.append(model)\n",
    "        if istest == True:\n",
    "            predict_y = model.predict(X_test)\n",
    "            # 这个地方概率值分档，应该拉出来看看\n",
    "            predict_label = [1 if i >0.4 else 0 for i in predict_y]\n",
    "            binary_classifier_metrics(y_test, predict_label, predict_y, False)  # every result\n",
    "            prediction += predict_y\n",
    "    if istest == True:\n",
    "        print('[End]     Average evaluation score: ')\n",
    "        predict_label = [1 if i >0.4 else 0 for i in prediction/5]  # average result\n",
    "        binary_classifier_metrics(y_test, predict_label, predict_y)\n",
    "        sta_bins = segment_statistic(y_test, prediction/5)\n",
    "        return models, importances, predict_y, sta_bins\n",
    "    else:\n",
    "        return models, importances\n",
    "# \n",
    "train_cols = X_train.columns\n",
    "train_x, val_x, train_y, val_y = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
    "models, importances, prediction, sta_bins = validation_prediction_lgb(train_x, train_y,\n",
    "                                                            train_cols, X_test=val_x, y_test=val_y, istest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alleged-vienna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['early_return_amount_3mon',\n",
       " 'work_year',\n",
       " 'censor_status',\n",
       " 'del_in_18month',\n",
       " 'f3',\n",
       " 'early_return_amount',\n",
       " 'total_loan',\n",
       " 'issue_date',\n",
       " 'region',\n",
       " 'recircle_b',\n",
       " 'f0',\n",
       " 'pub_dero_bankrup',\n",
       " 'industry',\n",
       " 'f4',\n",
       " 'post_code',\n",
       " 'year_of_loan',\n",
       " 'f2',\n",
       " 'employer_type',\n",
       " 'recircle_u',\n",
       " 'initial_list_status',\n",
       " 'f1',\n",
       " 'early_return',\n",
       " 'debt_loan_ratio',\n",
       " 'interest',\n",
       " 'class',\n",
       " 'use',\n",
       " 'house_exist',\n",
       " 'isDefault',\n",
       " 'monthly_payment',\n",
       " 'title']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
