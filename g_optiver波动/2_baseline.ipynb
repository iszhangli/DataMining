{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "reflected-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pressed-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:/ZhangLI/Codes/DataSet/optiver-realized-volatility-prediction/'\n",
    "train = pd.read_csv(data_dir + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "becoming-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc wap\n",
    "def calc_wap(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])/(df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])/(df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "def log_return(list_stock_prices):\n",
    "    return np.log(list_stock_prices).diff()  # time_id之间时间点操作\n",
    "# 计算已实现波动率\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "    \n",
    "# 其他函数\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "functioning-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed # parallel computing to save time\n",
    "df = pd.DataFrame()\n",
    "stock_id = 1\n",
    "is_train = True\n",
    "if is_train:\n",
    "    file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "    file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "else:\n",
    "    file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "    file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "\n",
    "# preprocess feature\n",
    "df = pd.read_parquet(file_path_book)\n",
    "df['wap'] = calc_wap(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "accurate-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建特征\n",
    "def preprocessor_book(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    #calculate return etc\n",
    "    df['wap'] = calc_wap(df)\n",
    "    df['log_return'] = df.groupby('time_id')['wap'].apply(log_return)\n",
    "    \n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n",
    "    \n",
    "    df['wap_balance'] = abs(df['wap'] - df['wap2'])\n",
    "    \n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1'])/2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    #dict for aggregate\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'log_return2':[realized_volatility],\n",
    "        'wap_balance':[np.mean],\n",
    "        'price_spread':[np.mean],\n",
    "        'bid_spread':[np.mean],\n",
    "        'ask_spread':[np.mean],\n",
    "        'volume_imbalance':[np.mean],\n",
    "        'total_volume':[np.mean],\n",
    "        'wap':[np.mean],\n",
    "            }\n",
    "    #####groupby / all seconds\n",
    "    df_feature = pd.DataFrame(df.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n",
    "    \n",
    "    df_feature.columns = ['_'.join(col) for col in df_feature.columns] #time_id is changed to time_id_\n",
    "        \n",
    "    ######groupby / last XX seconds\n",
    "    last_seconds = [300]\n",
    "    \n",
    "    for second in last_seconds:\n",
    "        second = 600 - second\n",
    "    \n",
    "        df_feature_sec = pd.DataFrame(df.query(f'seconds_in_bucket >= {second}').groupby(['time_id']).agg(create_feature_dict)).reset_index()\n",
    "        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns] #time_id is changed to time_id_\n",
    "     \n",
    "        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n",
    "        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n",
    "        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n",
    "    \n",
    "    #create row_id\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature = df_feature.drop(['time_id_'],axis=1)\n",
    "    \n",
    "    return df_feature\n",
    "def preprocessor_trade(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    \n",
    "    \n",
    "    aggregate_dictionary = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.mean],\n",
    "    }\n",
    "    \n",
    "    df_feature = df.groupby('time_id').agg(aggregate_dictionary)\n",
    "    \n",
    "    df_feature = df_feature.reset_index()\n",
    "    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "    \n",
    "    ######groupby / last XX seconds\n",
    "    last_seconds = [300]\n",
    "    \n",
    "    for second in last_seconds:\n",
    "        second = 600 - second\n",
    "    \n",
    "        df_feature_sec = df.query(f'seconds_in_bucket >= {second}').groupby('time_id').agg(aggregate_dictionary)\n",
    "        df_feature_sec = df_feature_sec.reset_index()\n",
    "        \n",
    "        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns]\n",
    "        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n",
    "        \n",
    "        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n",
    "        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature = df_feature.drop(['trade_time_id_'],axis=1)\n",
    "    \n",
    "    return df_feature\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    from joblib import Parallel, delayed # parallel computing to save time\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    def for_joblib(stock_id):\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "            \n",
    "        df_tmp = pd.merge(preprocessor_book(file_path_book),preprocessor_trade(file_path_trade),on='row_id',how='left')\n",
    "     \n",
    "        return pd.concat([df,df_tmp])\n",
    "    \n",
    "    df = Parallel(n_jobs=-1, verbose=1)(\n",
    "        delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n",
    "        )\n",
    "    df = pd.concat(df,ignore_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tender-lithuania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  26,  27,  28,\n",
       "        29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "        42,  43,  44,  46,  47,  48,  50,  51,  52,  53,  55,  56,  58,\n",
       "        59,  60,  61,  62,  63,  64,  66,  67,  68,  69,  70,  72,  73,\n",
       "        74,  75,  76,  77,  78,  80,  81,  82,  83,  84,  85,  86,  87,\n",
       "        88,  89,  90,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102,\n",
       "       103, 104, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       118, 119, 120, 122, 123, 124, 125, 126], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练集\n",
    "train = pd.read_csv(data_dir + 'train.csv')\n",
    "train_ids = train.stock_id.unique()\n",
    "df_train = preprocessor(list_stock_ids= train_ids, is_train = True)\n",
    "train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "train = train[['row_id','target']]\n",
    "df_train = train.merge(df_train, on = ['row_id'], how = 'left')\n",
    "\n",
    "# 测试集\n",
    "test = pd.read_csv(data_dir + 'test.csv')\n",
    "test_ids = test.stock_id.unique()\n",
    "df_test = preprocessor(list_stock_ids= test_ids, is_train = False)\n",
    "df_test = test.merge(df_test, on = ['row_id'], how = 'left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
