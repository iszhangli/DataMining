{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "completed-chile",
   "metadata": {},
   "source": [
    "# 一、指标统计\n",
    "### 1、分段统计函数\n",
    "* `segment_statistic(y_test=None, prob_y=None, bins=None)`\n",
    "\n",
    "### 2、二分类评估指标\n",
    "* `binary_classifier_metrics(test_labels, predict_labels, predict_prob, show_flag=True)`\n",
    "* 简单版\n",
    "\n",
    "### 3、其他\n",
    "* `show_feature_importance` Lightgbm特征重要性\n",
    "* `plt_feature_importance` \n",
    "* `plot_heatmap` 相关系数热力图\n",
    "* `create_roc` AUC可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "compound-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score, confusion_matrix, average_precision_score\n",
    "# 二分类评估指标\n",
    "# roc_auc_score = roc_curve + auc\n",
    "# test_labels: true label\n",
    "# predict_labels: predict lables\n",
    "# predict_prob: predict output is prob\n",
    "def binary_classifier_metrics(test_labels, predict_labels, predict_prob, show_flag=True):  # 评价标准\n",
    "    accuracy = accuracy_score(test_labels, predict_labels)  # accuracy_score准确率\n",
    "    precision = precision_score(test_labels, predict_labels)  # precision_score精确率\n",
    "    recall = recall_score(test_labels, predict_labels)  # recall_score召回率\n",
    "    f1_measure = f1_score(test_labels, predict_labels)  # f1_score  f1得分\n",
    "    confusionMatrix = confusion_matrix(test_labels, predict_labels)  # confusion_matrix  混淆矩阵\n",
    "    fpr, tpr, threshold = roc_curve(test_labels, predict_prob, pos_label=1)  # roc_curve ROC曲线\n",
    "    Auc = auc(fpr, tpr) \n",
    "    MAP = average_precision_score(test_labels, predict_prob)  # average_precision_score\n",
    "\n",
    "    TP, FP, FN, TN = confusionMatrix[1, 1], confusionMatrix[0, 1], confusionMatrix[1, 0], confusionMatrix[0, 0]\n",
    "    if show_flag is True:\n",
    "        print(\"------------------------- \")\n",
    "        print(\"row: precision | col: recall \")\n",
    "        print(\"confusion matrix:\")\n",
    "        print(\"------------------------- \")\n",
    "        print(\"| TP: %5d | FP: %5d | P: %5d |\" % (TP, FP, TP+FP))\n",
    "        print(\"----------------------- \")\n",
    "        print(\"| FN: %5d | TN: %5d | R: %.3f|\" % (FN, TN, (TP+FP)/len(test_labels)))\n",
    "        print(\"----------------------- \")\n",
    "        print(\"| T: %5d  | R: %.3f | N: %5d |\" % (TP+FN, (TP+FN)/len(test_labels), len(test_labels)))\n",
    "        print(\" ------------------------- \")\n",
    "        print(\"Accuracy:       %.2f%%\" % (accuracy * 100))\n",
    "        print(\"Precision:      %.2f%%\" % (precision * 100))\n",
    "        print(\"Recall:         %.2f%%\" % (recall * 100))\n",
    "        print(\"F1-measure:     %.2f%%\" % (f1_measure * 100))\n",
    "        print(\"AUC:            %.2f%%\" % (Auc * 100))\n",
    "        print(\"MAP:            %.2f%%\" % (MAP * 100))\n",
    "        print(\"------------------------- \")\n",
    "    return recall, precision, f1_measure\n",
    "\n",
    "def binary_classifier_metrics(test_labels, predict_labels, predict_prob):  # 评价标准\n",
    "    accuracy = accuracy_score(test_labels, predict_labels)  # accuracy_score准确率\n",
    "    precision = precision_score(test_labels, predict_labels)  # precision_score精确率\n",
    "    recall = recall_score(test_labels, predict_labels)  # recall_score召回率\n",
    "    f1_measure = f1_score(test_labels, predict_labels)  # f1_score  f1得分\n",
    "    auc = roc_auc_score(test_labels, predict_prob)\n",
    "    print(\"Accuracy:       %.2f%%\" % (accuracy * 100))\n",
    "    print(\"Precision:      %.2f%%\" % (precision * 100))\n",
    "    print(\"Recall:         %.2f%%\" % (recall * 100))\n",
    "    print(\"F1-measure:     %.2f%%\" % (f1_measure * 100))\n",
    "    print(\"AUC:            %.2f%%\" % (auc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "satisfactory-binding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分段统计函数\n",
    "# y_test: ture label\n",
    "# pro_y: predict probability\n",
    "def segment_statistic(y_test=None, prob_y=None, bins=None):\n",
    "    if bins is None:\n",
    "        bins = np.arange(0, 1.1, 0.1)\n",
    "    new_df = pd.DataFrame({'y_true': y_test, 'prob_y':prob_y})\n",
    "    new_df['bins'] = pd.cut(new_df['prob_y'], bins)\n",
    "    stra_df = new_df.groupby('bins').agg({'bins':'count', 'y_true':'sum'})\n",
    "    stra_df.rename(columns={'bins':'pred_cnt', 'y_true':'real_unsat_cnt'}, inplace=True)\n",
    "    stra_df = stra_df.sort_index(ascending=False)\n",
    "    stra_df['pred_unsat_cnt_p'] = stra_df['pred_cnt'].cumsum()\n",
    "    stra_df['recall_preson'] = stra_df['pred_unsat_cnt_p'] / stra_df['pred_cnt'].sum()\n",
    "    stra_df['real_unsat_cnt_tp'] = stra_df['real_unsat_cnt'].cumsum()\n",
    "    stra_df['recall'] = stra_df['real_unsat_cnt_tp'] / stra_df['real_unsat_cnt'].sum()\n",
    "    stra_df['precision'] = stra_df['real_unsat_cnt_tp'] / stra_df['pred_unsat_cnt_p']\n",
    "    return stra_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. the importance of feature \n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "feature_imp = list()\n",
    "for i in range(0, importances.shape[0]//5):\n",
    "    m_df = list()\n",
    "    m_df.append(importances.iloc[i,:].feature)\n",
    "    m_df += list(importances.loc[[i]].mean().values)\n",
    "    feature_imp.append(m_df)\n",
    "imp_df = pd.DataFrame(feature_imp, columns=['feature', 'split', 'gain', 'fold'])\n",
    "sort_imp_df = imp_df.sort_values(by=['gain'], ascending=False)\n",
    "sort_imp_df[sort_imp_df.gain>20]\n",
    "\n",
    "# 2. the importance of feature\n",
    "def show_feature_importance(model, usage_col):\n",
    "    import_df = pd.DataFrame()\n",
    "    import_df['feature'] = usage_col\n",
    "    import_df['split'] = model.feature_importance()\n",
    "    import_df['gain'] = model.feature_importance(importance_type='gain')\n",
    "    import_df = import_df.sort_values(by=['gain'], ascending=False)\n",
    "    return import_df\n",
    "\n",
    "def plt_feature_importance(model):\n",
    "    lgb.plot_importance(model, ax=ax, \n",
    "                        title='Feature Importance', \n",
    "                        xlabel='Information Gain', \n",
    "                        ylabel='Feature Name', \n",
    "                        importance_type='gain', \n",
    "                        max_num_features=10, \n",
    "                        grid=False, \n",
    "                        precision=3, \n",
    "                        height=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解决余弦相似度计算值缺失的问题\n",
    "# 去中心化的余弦相似度计算\n",
    "# 1. 相关系数取值一般在-1~1之间\n",
    "# 2. 绝对值越接近1说明变量之间的线性关系越强，绝对值越接近0说明变量间线性关系越弱。\n",
    "# 3. ≥0.8高度相关，0.5~0.8中度相关，0.3~0.5低度相关，＜0.3相关关系极弱可视为不相关。\n",
    "def plot_heatmap(dataframe):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    corr_df = dataframe\n",
    "    mcorr = corr_df.corr(method=\"spearman\")  \n",
    "\n",
    "    ax = plt.subplots(figsize=(30, 25)) #调整画布大小\n",
    "    mask = np.zeros_like(mcorr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)  # 颜色分布\n",
    "    ax = sns.heatmap(mcorr, mask=mask, cmap=cmap, annot=True, fmt='.1f')#画热力图   annot=True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization AUC\n",
    "# AUC可视化\n",
    "import matplotlib.pyplot as plt\n",
    "def create_roc(y_test, y_proba):\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.title('ROC Validation')\n",
    "    plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
