{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fifty-night",
   "metadata": {},
   "source": [
    "# Classficition Baseline\n",
    "1. LogisticRegression\n",
    "2. KNeighborsClassifier\n",
    "3. RandomForestClassifier\n",
    "4. tree.DecisionTreeClassifier()\n",
    "5. svm.SVC()\n",
    "6. Xgboost\n",
    "7. Lightgbm\n",
    "8. Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "protected-recognition",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f9387bd45d5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_train_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# ------------------------ LogisticRegression -------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train_x' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree,svm\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "df_train_x = train_data[feature_cols]\n",
    "df_train_y = train_data['label']\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_x, df_train_y, test_size=0.20, random_state=42)\n",
    "# ------------------------ LogisticRegression -------\n",
    "print(\"----------LogisticRegression--------------- \")\n",
    "clf2 = LogisticRegression()\n",
    "clf2.fit(x_train, y_train)\n",
    "lr_y_pred = clf2.predict(x_test)\n",
    "binary_classifier_metrics(y_test,lr_y_pred, lr_y_pred)\n",
    "# ------------------------KNeighborsClassifier -------\n",
    "print(\"----------KNeighborsClassifier--------------- \")\n",
    "clf3 = KNeighborsClassifier(5)\n",
    "clf3.fit(x_train, y_train)\n",
    "knc_y_pred = clf3.predict(x_test)\n",
    "binary_classifier_metrics(y_test,knc_y_pred, knc_y_pred)\n",
    "# -----------------------svm.SVC ---------------------\n",
    "print(\"----------SVC--------- ------ \")\n",
    "clf5 = svm.SVC()\n",
    "clf5.fit(x_train, y_train)\n",
    "svm_y_pred = clf5.predict(x_test)\n",
    "binary_classifier_metrics(y_test,svm_y_pred, svm_y_pred)\n",
    "# ----------------------tree.DecisionTreeClassifier---\n",
    "print(\"----------DecisionTreeClassifier--------------- \")\n",
    "clf4 = tree.DecisionTreeClassifier()\n",
    "clf4 = clf4.fit(x_train, y_train)\n",
    "dtc_y_pred = clf4.predict(x_test)\n",
    "binary_classifier_metrics(y_test,dtc_y_pred, dtc_y_pred)\n",
    "# -------------------- RandomForestClassifier --------\n",
    "print(\"----------RandomForestClassifier--------------- \")\n",
    "clf1 = RandomForestClassifier()\n",
    "clf1.fit(x_train, y_train)\n",
    "rfc_y_pred = clf1.predict(x_test)\n",
    "binary_classifier_metrics(y_test,rfc_y_pred, rfc_y_pred) \n",
    "# -------------------- Lightgbm ----------------------\n",
    "print(\"----------lightgbm--------------- \")\n",
    "train_data_l = lgb.Dataset(x_train, label=y_train) \n",
    "valid_data_l = lgb.Dataset(x_test, label=y_test)\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbose': -1,\n",
    "    'metric': 'auc',\n",
    "}\n",
    "model = lgb.train(params, train_set=train_data_l, num_boost_round=500, verbose_eval=100, valid_sets=[train_data_l, valid_data_l], early_stopping_rounds=20) \n",
    "predict_y = model.predict(x_test) # model['cvbooster']\n",
    "predict_label = [1 if i >0.5 else 0 for i in predict_y]\n",
    "binary_classifier_metrics(y_test, predict_label, predict_y)\n",
    "# -------------------- xgboost ----------------------\n",
    "print(\"----------xgboost--------------- \")\n",
    "train_data_x = xgb.DMatrix(x_train,label=y_train)   \n",
    "valid_data_x = xgb.DMatrix(x_test, label=y_test)  \n",
    "param = {\n",
    "    'objective':'binary:logistic' \n",
    "}    \n",
    "bst = xgb.train(param, dtrain=train_data_x, num_boost_round=500, evals=[(valid_data_x,'eval'), (train_data_x,'train')], verbose_eval=100, early_stopping_rounds=20)  \n",
    "predict_y = bst.predict(xgb.DMatrix(x_test)) \n",
    "predict_label = [1 if i >0.5 else 0 for i in predict_y]\n",
    "binary_classifier_metrics(y_test, predict_label, predict_y)\n",
    "# -------------------- Catboost ----------------------\n",
    "print(\"----------Catboost--------------- \")\n",
    "train_pool = cb.Pool(x_train,label=y_train)\n",
    "test_pool = cb.Pool(x_test, label=y_test)\n",
    "param = {\n",
    "    'objective':'Logloss' \n",
    "} \n",
    "ctb = cb.train(params=param, dtrain=train_pool, num_boost_round=500, eval_set=[test_pool, train_pool], verbose_eval=100, early_stopping_rounds=20)\n",
    "predict_y = ctb.predict(cb.Pool(x_test)) \n",
    "predict_label = [1 if i >0.5 else 0 for i in predict_y]\n",
    "binary_classifier_metrics(y_test, predict_label, predict_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score, confusion_matrix, average_precision_score\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "def validation_prediction_lgb(X, y, feature_names, ratio=1, X_test=None, y_test=None, istest=False):\n",
    "    n_fold = 5\n",
    "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': -1,\n",
    "        'metric': 'auc',\n",
    "        'is_unbalance': False,\n",
    "        'boost_from_average': False,\n",
    "    }\n",
    "\n",
    "    importances = pd.DataFrame()\n",
    "    if istest:\n",
    "        prediction = np.zeros(len(X_test))\n",
    "    models = []\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_train, X_valid = X[train_index], X[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        weights = [ratio if val == 1 else 1 for val in y_train]\n",
    "        train_data = lgb.Dataset(X_train, label=y_train, weight=weights)  # free_raw_data=True\n",
    "        valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "        model = lgb.train(params, train_data, num_boost_round=1000,\n",
    "                          valid_sets=[train_data, valid_data], verbose_eval=250, early_stopping_rounds=100)\n",
    "        \n",
    "        imp_df = pd.DataFrame()\n",
    "        imp_df['feature'] = feature_names\n",
    "        imp_df['split'] = model.feature_importance()\n",
    "        imp_df['gain'] = model.feature_importance(importance_type='gain')\n",
    "        imp_df['fold'] = fold_n + 1\n",
    "        importances = pd.concat([importances, imp_df], axis=0)\n",
    "        models.append(model)\n",
    "        if istest == True:\n",
    "            predict_y = model.predict(X_test)\n",
    "            # 这个地方概率值分档，应该拉出来看看\n",
    "            predict_label = [1 if i >0.4 else 0 for i in predict_y]\n",
    "            binary_classifier_metrics(y_test, predict_label, predict_y, False)  # every result\n",
    "            prediction += predict_y\n",
    "    if istest == True:\n",
    "        print('[End]     Average evaluation score: ')\n",
    "        predict_label = [1 if i >0.4 else 0 for i in prediction/5]  # average result\n",
    "        binary_classifier_metrics(y_test, predict_label, predict_y)\n",
    "        sta_bins = segment_statistic(y_test, prediction/5)\n",
    "        return models, importances, predict_y, sta_bins\n",
    "    else:\n",
    "        return models, importances\n",
    "train_df['target'] = train_df.mou_score*0.4 + train_df.dou_score*0.6\n",
    "train_df['target'] = train_df.target.apply(lambda x: 1 if x<9 else 0)\n",
    "train_cols = [col for col in train_df.columns if col not in ['mou_score', 'dou_score', 'target']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df[train_cols], train_df['target'], test_size=0.30, random_state=42)\n",
    "\n",
    "models, importances, prediction, sta_bins = validation_prediction_lgb(X_train.values, y_train.values,\n",
    "                                                            train_cols, X_test=X_test.values, y_test=y_test.values, istest=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_prediction_lgb_cv(X_train=None, y_train=None, X_test=None, y_test=None):\n",
    "    weights = [1 if val == 1 else 1 for val in y_train]\n",
    "    train_data = lgb.Dataset(X_train, label=y_train, weight=weights)  # free_raw_data=True\n",
    "    valid_data = lgb.Dataset(X_test, label=y_test)\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': -1,\n",
    "        'metric': 'auc',\n",
    "        'is_unbalance': False,\n",
    "        'boost_from_average': False,\n",
    "    }\n",
    "\n",
    "    model = lgb.cv(params, train_set=train_data, num_boost_round=500, verbose_eval=100, metrics='auc', eval_train_metric=True, return_cvbooster=True)\n",
    "    print(model['cvbooster'])\n",
    "    predict_y = model['cvbooster'].predict(X_test)\n",
    "    np.mean(predict_y)\n",
    "    predict_y = np.mean(predict_y, axis=0)\n",
    "#     print(predict_y[0])\n",
    "    predict_label = [1 if i >0.4 else 0 for i in predict_y]\n",
    "    binary_classifier_metrics(y_test, predict_label, predict_y)  # every result\n",
    "train_df['target'] = train_df.mou_score*0.4 + train_df.dou_score*0.6\n",
    "train_df['target'] = train_df.target.apply(lambda x: 1 if x<9 else 0)\n",
    "train_cols = [col for col in train_df.columns if col not in ['mou_score', 'dou_score', 'target']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df[train_cols], train_df['target'], test_size=0.30, random_state=42)\n",
    "validation_prediction_lgb_cv(X_train.values, y_train.values,X_test.values,y_test.values)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-distance",
   "metadata": {},
   "source": [
    "# Regression Baseline\n",
    "1. Linear\n",
    "2. ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
