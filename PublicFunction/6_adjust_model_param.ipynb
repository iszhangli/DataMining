{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "improved-toilet",
   "metadata": {},
   "source": [
    "## 调参\n",
    "## 调节时候过拟合\n",
    "### 一、手动调参\n",
    "1. adjustment_xgb_parameters(x_train=None, y_train=None, fig_ylim=3)\n",
    "2. adjust_lgb_parameters(x_train=None, y_train=None, x_test=None, y_test=None, xlim=None)\n",
    "\n",
    "### 二、网格搜索\n",
    "\n",
    "### 三、调参顺序\n",
    "1. n_estimators\n",
    "2. eta\n",
    "3. gamma\n",
    "4. max_depth\n",
    "5. 采样\n",
    "6. 抽样参数（纵向抽样影响更大）\n",
    "7. 正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "marked-people",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ee03ec8631eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"xx-large\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m \u001b[0madjust_lgbc_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "# 学习曲线调参 一般的函数\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "def adjust_lgbc_param(train_x=None, train_y=None, test_x=None, test_y=None, metric='accuracy'):\n",
    "    Xtrain, Ytrain, Xtest, Ytest = train_x, train_y, test_x, test_y\n",
    "\n",
    "    acc_train, acc_test = [], []\n",
    "    pre_train, pre_test = [], []\n",
    "    recall_train, recall_test = [], []\n",
    "    f1_train, f1_test = [], []\n",
    "    auc_train, auc_test = [], []\n",
    "    map_train, map_test = [], [] \n",
    "    # 调优精度\n",
    "    start, end, step = 1, 10, 1  \n",
    "    n_cv = 5\n",
    "    \n",
    "    for i in range(start,end,step):\n",
    "        lgbc = LGBMClassifier(\n",
    "                             boosting_type='gbdt',\n",
    "                             class_weight='balanced',\n",
    "                             objective='binary',\n",
    "                             colsample_bytree=1.0,\n",
    "                             importance_type='split', # 和feature_importance 配合使用\n",
    "                             n_estimators=81,\n",
    "                             learning_rate=0.1,\n",
    "                             max_depth=i,\n",
    "                             min_child_samples=20,\n",
    "                             min_child_weight=0.001,\n",
    "                             min_split_gain=0.0,\n",
    "                             num_leaves=31,\n",
    "                             reg_alpha=0.0,\n",
    "                             reg_lambda=0.0,\n",
    "                             silent=True,\n",
    "                             subsample=1.0,\n",
    "                             subsample_for_bin=200000,\n",
    "                             subsample_freq=0)\n",
    "        score = cross_val_score(lgbc,Xtrain,Ytrain,cv=n_cv, scoring='accuracy').mean()\n",
    "        acc_train.append(score)\n",
    "        score = cross_val_score(lgbc,Xtrain,Ytrain,cv=n_cv, scoring='precision').mean()\n",
    "        pre_train.append(score)\n",
    "        score = cross_val_score(lgbc,Xtrain,Ytrain,cv=n_cv, scoring='recall').mean()\n",
    "        recall_train.append(score)\n",
    "        score = cross_val_score(lgbc,Xtrain,Ytrain,cv=n_cv, scoring='f1').mean()\n",
    "        f1_train.append(score)\n",
    "        score = cross_val_score(lgbc,Xtrain,Ytrain,cv=n_cv, scoring='roc_auc').mean()\n",
    "        auc_train.append(score)\n",
    "\n",
    "    print('max(accuracy):  %f, index: %d' % (max(acc_train),(acc_train.index(max(acc_train))*step)+1+start))\n",
    "    print('max(precision): %f, index: %d' % (max(pre_train),(pre_train.index(max(pre_train))*step)+1+start))\n",
    "    print('max(recall):    %f, index: %d' % (max(recall_train),(recall_train.index(max(recall_train))*step)+1+start))\n",
    "    print('max(f1):        %f, index: %d' % (max(f1_train),(f1_train.index(max(f1_train))*step)+1+start))\n",
    "    print('max(roc_auc):   %f, index: %d' % (max(auc_train),(auc_train.index(max(auc_train))*step)+1+start))\n",
    "    \n",
    "    fig = plt.figure(figsize=[20,20])\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_title('accuracy')\n",
    "    ax.plot(range(start,end,step),acc_train, c=\"red\",label=\"accuracy\")\n",
    "    ax.plot(range(start,end,step),pre_train, c=\"green\",label=\"precision\")\n",
    "    ax.plot(range(start,end,step),recall_train, c=\"blue\",label=\"recall\")\n",
    "    ax.plot(range(start,end,step),f1_train, c=\"orange\",label=\"f1\")\n",
    "    ax.plot(range(start,end,step),auc_train, c=\"pink\",label=\"auc\")\n",
    "    ax.legend(fontsize=\"xx-large\")\n",
    "    plt.show()\n",
    "adjust_lgbc_param(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjustment_xgb_parameters(x_train=None, y_train=None, fig_ylim=3):\n",
    "    \"\"\"x_train, y_train\"\"\"\n",
    "    fig,ax = plt.subplots(1,figsize=(15,8))\n",
    "    ax.set_ylim(top=fig_ylim)\n",
    "    ax.grid()\n",
    "\n",
    "    dfull = xgb.DMatrix(x_train,y_train) \n",
    "    # Init parameter\n",
    "    param1 = {'verbosity':1, # -- global parameter \n",
    "              'objective':'binary:logistic',  # -- task parameter\n",
    "              'eval_metric':'auc',\n",
    "              \"subsample\":1,  # -- tree booster parameter\n",
    "              \"max_depth\":6,\n",
    "              \"eta\":0.3,\n",
    "              \"gamma\":0,\n",
    "              \"lambda\":1,\n",
    "              \"alpha\":0,\n",
    "              \"colsample_bytree\":1,\n",
    "              \"colsample_bylevel\":1,\n",
    "              \"colsample_bynode\":1,\n",
    "            }\n",
    "    num_round = 200\n",
    "    cvresult1 = xgb.cv(params=param1, dtrain=dfull, num_boost_round=num_round,nfold=5)\n",
    "    ax.plot(range(1,num_round+1),cvresult1.iloc[:,0],c=\"red\",label=\"train,original\")\n",
    "    ax.plot(range(1,num_round+1),cvresult1.iloc[:,2],c=\"orange\",label=\"test,original\")\n",
    "\n",
    "    # Usable parameter\n",
    "    param2 = {'verbosity':1, \n",
    "              'objective':'binary:logistic', \n",
    "              'eval_metric':'auc'\n",
    "             }\n",
    "    num_round = 200\n",
    "    cvresult2 = xgb.cv(params=param1, dtrain=dfull, num_boost_round=num_round,nfold=5)\n",
    "    ax.plot(range(1,num_round+1),cvresult2.iloc[:,0],c=\"green\",label=\"train,last\")\n",
    "    ax.plot(range(1,num_round+1),cvresult2.iloc[:,2],c=\"blue\",label=\"test,last\")\n",
    "\n",
    "    # Adjusting parameter\n",
    "    param3 = {'verbosity':1,  \n",
    "              'objective':'binary:logistic',\n",
    "              'eval_metric':'auc'\n",
    "             }\n",
    "    num_round = 200\n",
    "    cvresult3 = xgb.cv(params=param1, dtrain=dfull, num_boost_round=num_round,nfold=5)\n",
    "    ax.plot(range(1,num_round+1),cvresult3.iloc[:,0],c=\"gray\",label=\"train,this\")\n",
    "    ax.plot(range(1,num_round+1),cvresult3.iloc[:,2],c=\"pink\",label=\"test,this\")\n",
    "    ax.legend(fontsize=\"xx-large\")\n",
    "    plt.show()\n",
    "adjustment_xgb_parameters(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the param of lightgbm   use eval_result = {} \n",
    "def adjust_lgb_parameters(x_train=None, y_train=None, x_test=None, y_test=None, xlim=None):\n",
    "    # auc / binary_logloss(binary) / binary_error \n",
    "    train_data_l = lgb.Dataset(x_train, label=y_train) \n",
    "    valid_data_l = lgb.Dataset(x_test, label=y_test)\n",
    "\n",
    "    fig = plt.figure(figsize=(20,16))\n",
    "    ax = fig.add_subplot(211)\n",
    "    xlim = xlim\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': -1,\n",
    "        'metric': 'binary_logloss',\n",
    "        'n_estimators': 100,\n",
    "    }\n",
    "    evals_result_ori = {}\n",
    "    model = lgb.train(params, train_set=train_data_l, verbose_eval=100, valid_sets=[train_data_l, valid_data_l], evals_result=evals_result_ori) \n",
    "    lgb.plot_metric(evals_result_ori, metric=params['metric'], ax=ax, xlim=xlim)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': -1,\n",
    "        'metric': 'binary_logloss',\n",
    "        'n_estimators': 100,\n",
    "        'learning_rate': 0.03,\n",
    "        'num_leaves': 21,\n",
    "        'reg_alpha': 0.2,\n",
    "        'reg_lambda': 0.2,\n",
    "    }\n",
    "    evals_result_last = {}\n",
    "    model = lgb.train(params, train_set=train_data_l, verbose_eval=100, valid_sets=[train_data_l, valid_data_l], evals_result=evals_result_last)\n",
    "    lgb.plot_metric(evals_result_last, metric=params['metric'], ax=ax, xlim=xlim)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': -1,\n",
    "        'metric': 'binary_logloss',\n",
    "        'n_estimators': 100,\n",
    "        'learning_rate': 0.03,\n",
    "        'num_leaves': 21,\n",
    "        'reg_alpha': 0.2,\n",
    "        'reg_lambda': 0.2,\n",
    "        'subsample': 0.5\n",
    "    }\n",
    "    evals_result_this = {}\n",
    "    model = lgb.train(params, train_set=train_data_l, verbose_eval=100, valid_sets=[train_data_l, valid_data_l], evals_result=evals_result_this)\n",
    "    lgb.plot_metric(evals_result_this, metric=params['metric'], ax=ax, xlim=xlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-hardware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust catboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网格搜索\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': [5, 10, 15, 20, 25],\n",
    "    'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15],\n",
    "    'n_estimators': [500, 1000, 2000, 3000, 5000],\n",
    "    'min_child_weight': [0, 2, 5, 10, 20],\n",
    "    'max_delta_step': [0, 0.2, 0.6, 1, 2],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.85, 0.95],\n",
    "    'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'reg_alpha': [0, 0.25, 0.5, 0.75, 1],\n",
    "    'reg_lambda': [0.2, 0.4, 0.6, 0.8, 1],\n",
    "    'scale_pos_weight': [0.2, 0.4, 0.6, 0.8, 1]\n",
    "}\n",
    "\n",
    "xlf = xgb.XGBClassifier(max_depth=10,\n",
    "                        learning_rate=0.01,\n",
    "                        n_estimators=2000,\n",
    "                        silent=True,\n",
    "                        objective='binary:logistic',\n",
    "                        nthread=-1,\n",
    "                        gamma=0,\n",
    "                        min_child_weight=1,\n",
    "                        max_delta_step=0,\n",
    "                        subsample=0.85,\n",
    "                        colsample_bytree=0.7,\n",
    "                        colsample_bylevel=1,\n",
    "                        reg_alpha=0,\n",
    "                        reg_lambda=1,\n",
    "                        scale_pos_weight=1,\n",
    "                        seed=1440,\n",
    "                        missing=None\n",
    "                       )\n",
    "\n",
    "gsearch = GridSearchCV(xlf, param_grid=parameters, scoring='f1', cv=3, verbose=1)\n",
    "gsearch.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % gsearch.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = gsearch.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
